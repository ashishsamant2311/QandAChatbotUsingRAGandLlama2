{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7676adIK3OkH"
   },
   "source": [
    "<h4> Installing Necessary Libraries </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "1fDTXXrU2zVG"
   },
   "outputs": [],
   "source": [
    "!pip install transformers -q\n",
    "!pip install bitsandbytes -q\n",
    "!pip install -U bitsandbytes -q\n",
    "!pip install -U langchain-community -q\n",
    "!pip install pypdf pymupdf -q\n",
    "!pip install chromadb -q\n",
    "!pip install sentence_transformers -q\n",
    "!pip install huggingface_hub -q\n",
    "!pip install tqdm -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JxjKvjv44K-Y"
   },
   "source": [
    "<h4> Importing Libraries </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "payEBXk74I9j"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "import gc\n",
    "import sys\n",
    "import langchain\n",
    "from langchain.document_loaders import PyMuPDFLoader, PyPDFLoader\n",
    "import re\n",
    "import os\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import pipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from huggingface_hub import login\n",
    "from huggingface_hub import snapshot_download\n",
    "from tqdm import tqdm\n",
    "hf_token = '<Enter your token here>'\n",
    "login(token=hf_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZHBOuvb45Lsj"
   },
   "source": [
    "<h4> Variable Initialization </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "UI3iV2Va4_-o"
   },
   "outputs": [],
   "source": [
    "file_path = '/content/UK_EMPLOYEE_HANDBOOK_2022.pdf'\n",
    "# model_name = \"meta-llama/Llama-2-13b-chat-hf\"\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "# save_directory = \"./llama2-13b-chat-4bit\"\n",
    "save_directory = \"./\" + model_name.split('/')[1]\n",
    "persist_directory = './database/chroma/'\n",
    "pages_to_exclude = [1, 3, 4, 81, 82] #These pages consisted of Index and irrelevant pages\n",
    "chunk_size = 500\n",
    "chunk_overlap = 100\n",
    "max_new_tokens = 200\n",
    "\n",
    "prompt_template_str = \"\"\"[INST] Use the following pieces of context to answer the question at the end in ONE SENTENCE ONLY.\n",
    "If the context is not relevant to the question, then Helpful Answer : I don't know the answer to that\". Do NOT give any explanation.\n",
    "Don't try to make up an answer. Avoid Repetition.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer: [/INST]'\n",
    "\"\"\"\n",
    "\n",
    "# prompt_template_str = \"\"\"[INST] Use the following pieces of context to answer the question at the end in ONE SENTENCE ONLY.\n",
    "# If you don't know the answer, Helpful Answer : I don't know the answer to that\".\n",
    "# Don't try to make up an answer. Avoid Repetition.\n",
    "\n",
    "# Context: {context}\n",
    "\n",
    "# Question: {question}\n",
    "# Helpful Answer: [/INST]'\n",
    "# \"\"\"\n",
    "\n",
    "# prompt_template_str = \"\"\"\n",
    "# You are a helpful assistant. Use the following context to answer the question in one sentence immediately after \"Answer:\".\n",
    "# If you don't know the answer, just say \"I don't know.\" Do not repeat or add extra text. Don't start your answer with \"Please note that I\\'m just an AI and not a legal expert. However, based on the provided context,\". Just write a single sentence response\n",
    "\n",
    "# Context:\n",
    "# {context}\n",
    "\n",
    "# Question: {question}\n",
    "# Answer:\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "# prompt_template_str = \"\"\"\n",
    "# You are a helpful assistant. Use the following context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "# Context:\n",
    "# {context}\n",
    "\n",
    "# Question: {question}\n",
    "# Answer:\n",
    "# \"\"\"\n",
    "\n",
    "# prompt_template_str = \"\"\"You are a helpful assistant. Use the context given below to answer the question in one concise sentence. Do not include any options in the answer. If you don't know the answer, just say \"I don't know.\". Do not make up an answer.\n",
    "\n",
    "# Context:\n",
    "# {context}\n",
    "\n",
    "# Question: {question}\n",
    "# Answer:\n",
    "# \"\"\"\n",
    "\n",
    "# prompt_template_str = \"\"\"Use the following context to answer the question at the end.\n",
    "# If you don't know the answer, just say that you don't know.\n",
    "# Don't try to make up an answer. Avoid Repetition of same pointers.\n",
    "\n",
    "# Context: {context}\n",
    "\n",
    "# Question: {question}\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "VLnXFJyZOZh2",
    "outputId": "ca01996a-88c8-4568-ccca-3665b722ef1e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'./Mistral-7B-Instruct-v0.2'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a2LJFgAqAoBa"
   },
   "source": [
    "<h4> Globally Declaring the model and Tokenizer </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "M9mrTlQsAtD6"
   },
   "outputs": [],
   "source": [
    "model = None\n",
    "tokenizer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AlPH-aRRAvuB"
   },
   "source": [
    "<h4> Helper Functions </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "i0IFO0Vp-pdA"
   },
   "outputs": [],
   "source": [
    "# Function to Save the Downloaded Model and Tokenizer to Disk\n",
    "# def fnSaveModelandTokenizer(model_name, save_directory):\n",
    "#   quantization_config = BitsAndBytesConfig(\n",
    "#       load_in_4bit=True,  # Load the model in 4-bit precision\n",
    "#       bnb_4bit_use_double_quant=True,  # Use double quantization for better precision\n",
    "#       bnb_4bit_quant_type=\"nf4\",  # Use 4-bit NormalFloat quantization\n",
    "#       bnb_4bit_compute_dtype=torch.float16)  # Use FP16 for computation\n",
    "#   tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#   model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=quantization_config, device_map=\"auto\")\n",
    "#   model.save_pretrained(save_directory)\n",
    "#   tokenizer.save_pretrained(save_directory)\n",
    "#   torch.save(model.state_dict(), os.path.join(save_directory, \"pytorch_model.bin\"))\n",
    "#   model.config.save_pretrained(save_directory)\n",
    "#   print(f\"Model and tokenizer saved to '{save_directory}\")\n",
    "#   fnClearModelFromGPU(model)\n",
    "#   print(f\"Model Cleared from the GPU Memory\")\n",
    "\n",
    "def fnSaveModelandTokenizer(model_name, save_directory):\n",
    "  snapshot_download(repo_id=model_name, local_dir=save_directory)\n",
    "\n",
    "# Loading the saved model and Tokenizer from Disk\n",
    "def fnLoadModelandTokenizer(model_name, save_directory):\n",
    "  quantization_config = BitsAndBytesConfig(\n",
    "      load_in_4bit=True,  # Load the model in 4-bit precision\n",
    "      bnb_4bit_use_double_quant=True,  # Use double quantization for better precision\n",
    "      bnb_4bit_quant_type=\"nf4\",  # Use 4-bit NormalFloat quantization\n",
    "      bnb_4bit_compute_dtype=torch.float16)\n",
    "  print(\"Loading tokenizer...\")\n",
    "  tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
    "  print(\"Tokenizer successfully loaded.\")\n",
    "  from transformers import AutoConfig\n",
    "  config = AutoConfig.from_pretrained(save_directory)\n",
    "  # Initialize the model with quantization\n",
    "  print(\"Loading model...\")\n",
    "  model = AutoModelForCausalLM.from_pretrained(\n",
    "    save_directory,\n",
    "    config=config,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\")\n",
    "  return model, tokenizer\n",
    "\n",
    "# Function to Download the Model and Tokenizer from HuggingFace\n",
    "# def fnDownloadModelandTokenizer(model_name):\n",
    "#   quantization_config = BitsAndBytesConfig(\n",
    "#       load_in_4bit=True,  # Load the model in 4-bit precision\n",
    "#       bnb_4bit_use_double_quant=True,  # Use double quantization for better precision\n",
    "#       bnb_4bit_quant_type=\"nf4\",  # Use 4-bit NormalFloat quantization\n",
    "#       bnb_4bit_compute_dtype=torch.float16)  # Use FP16 for computation\n",
    "#   tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#   model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=quantization_config, device_map=\"auto\")\n",
    "#   return model,tokenizer\n",
    "\n",
    "# Function to Unload the model from GPU\n",
    "def fnClearModelFromGPU(model):\n",
    "  model.cpu()\n",
    "  del model\n",
    "  gc.collect()\n",
    "  torch.cuda.empty_cache()\n",
    "\n",
    "# Function to Clean the PDF text\n",
    "def clean_text(text):\n",
    "    # Remove pattern 1: \"<number> \\nMORGAN STANLEY | UK EMPLOYEE HANDBOOK 2022\"\n",
    "    text = re.sub(r'\\d+\\s+\\nMORGAN STANLEY \\| UK EMPLOYEE HANDBOOK 2022', '', text)\n",
    "\n",
    "    # Remove pattern 2: \"MORGAN STANLEY | UK EMPLOYEE HANDBOOK 2022\"\n",
    "    text = re.sub(r'MORGAN STANLEY \\| UK EMPLOYEE HANDBOOK 2022', '', text)\n",
    "\n",
    "    # Remove the first number\n",
    "    text = re.sub(r'^\\d+\\s+\\n', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove standalone numbers followed by newlines (e.g., \"6 \\n\")\n",
    "    text = re.sub(r'\\d+\\s+\\n', '', text)\n",
    "\n",
    "    # Replace all newline characters with a space\n",
    "    text = text.replace('\\n', ' ')\n",
    "\n",
    "    # Replace multiple spaces with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    # Remove leading/trailing whitespace\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "# Read the PDF and create and save Vector Embeddings\n",
    "def fnCreateVectorDB(file_path, persist_directory, pages_to_exclude):\n",
    "  loader = PyMuPDFLoader(file_path)\n",
    "  print('PDF Read Successfully')\n",
    "  documents = loader.load()\n",
    "  print(f'The document has {len(documents)} pages.')\n",
    "  filtered_documents = [doc for doc in documents if int(doc.metadata['page']) not in pages_to_exclude]\n",
    "  print(f'The document has {len(filtered_documents)} pages after removing irrelevant content.')\n",
    "  for doc in filtered_documents:\n",
    "      doc.page_content = clean_text(doc.page_content)\n",
    "  print(\"Text Cleaning completed\")\n",
    "  text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "  splits = text_splitter.split_documents(filtered_documents)\n",
    "  print(f\"The document has {len(splits)} chunks\")\n",
    "  embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "  vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings, persist_directory=persist_directory)\n",
    "  vectorstore.persist()\n",
    "  print(\"VectorDB created and saved successfully.\")\n",
    "  return vectorstore\n",
    "\n",
    "# Creating a prompt template to pass to Langchain\n",
    "def create_prompt_template():\n",
    "  return PromptTemplate(template=prompt_template_str, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "# Creating a compression retriever\n",
    "def create_compression_retriever(model, tokenizer, vector_store):\n",
    "    # Wrap the model in a LangChain-compatible LLM\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=0.1,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "    # Create the compressor\n",
    "    compressor = LLMChainExtractor.from_llm(llm)\n",
    "\n",
    "    # Create the compression retriever\n",
    "    compression_retriever = ContextualCompressionRetriever(\n",
    "        base_compressor=compressor,\n",
    "        base_retriever=vector_store.as_retriever(search_kwargs={\"k\": 5})  # Retrieve top 3 documents\n",
    "    )\n",
    "    return compression_retriever\n",
    "\n",
    "def create_vector_retriever(model, tokenizer):\n",
    "    # Wrap the model in a LangChain-compatible LLM\n",
    "    retriever = vector_store.as_retriever(search_type=\"similarity\",\n",
    "                                      #  metadata_field_info=metadata_field_info,\n",
    "                                      search_kwargs={\"k\": 2})\n",
    "    return retriever\n",
    "\n",
    "# Creating a Q and A chain\n",
    "def create_qa_chain(model, tokenizer, retriever):\n",
    "    # Wrap the model in a LangChain-compatible LLM\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=0.1,\n",
    "        top_p = 0.9,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "    llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "    prompt_template = create_prompt_template()\n",
    "\n",
    "    print(prompt_template.template)\n",
    "\n",
    "    # Create the Q&A chain\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        chain_type_kwargs={\"prompt\": prompt_template},\n",
    "        return_source_documents=True\n",
    "    )\n",
    "    return qa_chain\n",
    "\n",
    "# Function to Answer Questions from a Pandas Dataframe\n",
    "def answer_questions_from_dataframe(qa_chain, df):\n",
    "    answers = []\n",
    "    # Wrap the loop with tqdm to show progress\n",
    "    for question in tqdm(df[\"question\"], desc=\"Processing questions\"):\n",
    "        output = qa_chain.invoke(question)\n",
    "        answers.append(output[\"result\"])\n",
    "    # Apply extraction for each answer\n",
    "    answers = [extract_text_after_answer(ans) for ans in answers]\n",
    "    df[\"answer\"] = answers\n",
    "    return df\n",
    "\n",
    "# Extract the actual response and ignore the instruction and context\n",
    "def extract_text_after_answer(text):\n",
    "    keyword = \"[/INST]'\"\n",
    "    index = text.find(keyword)\n",
    "    if index == -1:\n",
    "        return \"\"\n",
    "    # Extract the text after \"[/INST]'\" and strip leading/trailing whitespace.\n",
    "    text = text[index + len(keyword):].strip()\n",
    "    text = post_process(text)\n",
    "    return text\n",
    "\n",
    "# Creating a new function to eliminate junk text\n",
    "def post_process(text: str) -> str:\n",
    "    # Remove everything after the first occurrence of \"\\n\\n\"\n",
    "    text = text.split(\"\\n\\n\", 1)[0]\n",
    "\n",
    "    # Remove text inside parentheses and square brackets\n",
    "    text = re.sub(r'\\(.*?\\)', '', text)  # Remove text in parentheses\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)  # Remove text in square brackets\n",
    "\n",
    "    text = re.sub(r\"'\\s*(?=[.?!])\", '', text)  # Remove apostrophes immediately before punctuation\n",
    "    text = re.sub(r\"'\\s*$\", '', text)           # Remove apostrophe if at the very end\n",
    "\n",
    "    # Clean up extra spaces that might result from removals\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 853,
     "referenced_widgets": [
      "dd9adda7d31640719195b4538b452662",
      "e3c61a199bfd4da089b0b54e4a643aea",
      "33b98ef55c1a4a1bb676c6a51b7f8670",
      "699e851f3fee4d0bbd58dfe0d252ab27",
      "d2cb0130cf354381bff55414343319e5",
      "3d56b25dc6dd452693ca9a00bb072bfe",
      "beacf1465f534b69808b7dc69556d101",
      "91df4c7544584f95a9b666fc78baea8e",
      "5420c748e6c24026ae414c9c3776e914",
      "476755eea2784de7be820a0e053d207b",
      "99742166c1354839a515bef183283b00",
      "6840ced2b4b244c882e1207d74eb190a",
      "b6ad585b3c904efa871b3b2466604dc5",
      "74fdd8ad3c1e468cafcda822311dac33",
      "089f9e36f6d647219148c50a2fcca208",
      "a8cf5136e63c4218b3f2e5b4bd19d7ef",
      "341304b593de4886bb55e99dd3d9373b",
      "129693438c2c4988ac392a896a15fe2f",
      "c785b3d26e484c75a3b7026bd4bcddc5",
      "b9a1ee1211f64d7eb92b8ab7ea79da75",
      "3c42212cd0e043059991e5c044c9caf1",
      "1963a0b1890446b5a6a4efa179648973",
      "30bb64f0950c41a4be940484e04abfc8",
      "ac42d341a83743f18bf3d3972a445604",
      "c304911de85349aaa59bcba1ecb3aa27",
      "b34ee4a515fe4ad4bfa0240c2af78c0c",
      "ba21148d84ef4f3593ecb61dc2b9d299",
      "2ace24d73e664a3a870c8c457e70ccda",
      "3c63c5bbd7764cddbf32ecd363b935f7",
      "9e8e4120f30b46b7a1048d1d95cfddd8",
      "575007529963462aae8a8572779d7710",
      "b6bf58ea23ed44f68b76e1148358957b",
      "11193504844448cfb74e51ded59e51d5",
      "de5c6ba4cc1a41c696f6c5c9e1a2a57e",
      "f8d0a3ec789a4942b24cd7c2fb891790",
      "f5d156e280ed4f07882e1f70d433bb37",
      "fbf7aeb80a7b48ad9e2ecd080d0154d7",
      "ff14e4647e5b49a483c16835a8b7355f",
      "988e74b192404ede9251fddb5e040980",
      "834ebc3201cc474997769bf460b08afd",
      "153d4cb76cb94bf0b20c3242e07afe08",
      "f84a0e7b593647ce9b0d4920a93eda74",
      "5b9846ef17bf484a8a2fff3614a68269",
      "094aebf4044746388964a5d307025674",
      "9c067e44d0c4474799f61dcdeea2e93b",
      "038a702eff5e4292ab0f8f0499963b08",
      "9c46f1f00b824fb280b48416c38d6cf8",
      "4de4ee6604404fea8c22c5e2fd8a1c5b",
      "a0e997833c8a4a9687c5d2459eca72d2",
      "e04717dd0d8b4473847b2315ded91e95",
      "f1502e3d49a84eb295daab82b3fc83d6",
      "0de4c2d810db47b6af669e3964a5ec6d",
      "aa6e298683f34b5aa1e881e49a090cd9",
      "21e07ae01587434a8e9f79aad2bb68e6",
      "e1b11591b574461fbc77c73f420c75cc",
      "59d3b98ddc8f43718d4290060f0bd763",
      "0577313d91344f3b87d4cc7b22d2e9b6",
      "9e0c48718f404cfdbe2938e2fbd4143e",
      "32c0d277e2c44594af27e2ddf3f217cc",
      "6e49618a8ebc4158ab2ad65bbf052e54",
      "6601da17dec045709a163443fc2584b3",
      "c31478434f2a41b59247b86d21986553",
      "058fb686d22d4445a471246f72f4f600",
      "e55743373fab461bb7c1424267dca44e",
      "9508d5c23dde43b4ab0cf8d64b9924ce",
      "4e80a99fe2c540c88ab1d8fd38dca1ed",
      "d4cb8ed4b3004d2eaf29a6826761c5d9",
      "d294ff5e7b8e4cb092a758876b262d4b",
      "3a118db57d4e47a1b52aa994b6705ab7",
      "cd04aa8a938b466c9295111108fb93c0",
      "c9326932a2d24a76b3e52be4c52c592e",
      "a55214bcdd314d9d8549eebf5dbbe734",
      "63aa0baf845e4dcf88e92091f966ff67",
      "1ae9fcd5b3ef412388d628a0f4ae8df8",
      "d94c13c1bde64ae694f1a814e849621f",
      "4d2ac46fdff3407b8b6fccfc9a783a59",
      "b06f7a93fcba4f54870e079fdf89b2c0",
      "93e98717b6b14a80803ef3f70531ce5e",
      "328d1a8a84934cf4bbb959c82c68453d",
      "3a2d9cc4aa734aaa851089b4e0b3a5d9",
      "6cbf423cedbd4a8582f69c2d6469f4da",
      "c3eb76e80a1045a0b431f6dff3d71b19",
      "f84705d07bfa481c973f42fcfa1a0da8",
      "40d598ed7d9d487a8a19b00f7cfd4758",
      "b5f8b7d57c61472fb804362ef3aec9f5",
      "e47e90f3589642b58152eb634dc9654c",
      "200875a95b9a487ea616102026b44d76",
      "23d2d52d234144618b2435921f3d0bc6",
      "9c17b1ba827c405db80c7acfbc3650f9",
      "c10f3f4b66a944f78678fc86dfbbcf27",
      "c146b0611c64491992415ebb7fb969d0",
      "7339e782e1f3495382a32ebda843231d",
      "6d35c1a072514d5484620f73d52b1707",
      "a2d291ac1cd8484d8d9d15ac941eb2d3",
      "f2fb5ad2f2d9472d9cb62ee8406a655d",
      "4a0235a2c63d4bd78b865f96c80587d9",
      "6fac065d911541c8b5d29185610a6f5f",
      "13343b2ad76a4f1da817280baea253ca",
      "5aa6ea09904148e989d6cce242dfe0b0",
      "17f6945b7a8d4811bfa4ce0fffcd9457",
      "d88f1eb4f1084cd68d29a3ed29bf00ba",
      "ae6e5a0dc98d47abbefb6e2597de2167",
      "25bc5d2f20fd487eb8f8ea4d4ab8d01c",
      "623ff0c0aca94ff2be5a889c90172d2a",
      "eb5a7027a8284c7db4f9d8e73d563408",
      "e848659ab35e44c694215546aeeea43a",
      "2d350f6cb7e0490f8796e3347da918c2",
      "b49d6c4790d8431285700be522c4cc62",
      "3a94d97cd8be42f3a8c2416066296bec",
      "64bb999afb8144c9b26b53a7eaa66159",
      "c4b84492006f4be0a90055fe40216808",
      "4def37eef07f4a4abdb8865181585eb5",
      "2eebc7e61bb44a9f8b3dd5091fd6942f",
      "b22aa662d4dc4806b53e26879b06e795",
      "1dadf940512d4ee199475d4350591671",
      "afecbf98cc8247a79fc181a2bfb58b9f",
      "daedf0e170e5426796905eb8f310fe45",
      "80db9f0707074ffb89c34fc4c26ebae9",
      "d5dacb5377fc4e569e4d3cf86e23f880",
      "310f52ec3aba4a1a9b97226271ceafb2",
      "48a95ba233414d14ae59ba41e640978e",
      "a5eb949c92a34cc09599e161497f90ac",
      "de533d97ffa144a89fa2a28b2c6736c5",
      "3abc378dad974276893c2e1c22a2e3bd",
      "e67dc4d4427e46a997e70a8c352858a0",
      "f46f022313da4fa1a21ac51b00f38cc3",
      "78217fb84cf1418aa2617a5d311913f9",
      "1325d5972c0d4dd29856a23316277d3d",
      "82487604b41c42bc83040c0802730d37",
      "5342c499db21456c9293d181e33f09bd",
      "5cc4df3eb1b649b59f51cbbed8f23ae9",
      "6894368e534449e6ac36ee9c8bf73e2c",
      "ce7a5d27a95944ec94b621fcd954883e",
      "4f4421348afd43cdba1686654b9fd8e2",
      "53f54ebcbab940be95e2f1572186d660",
      "991be378d90e45abba53d7ebaac07905",
      "fd51d7e4ff8e4172a717762947cb246c",
      "30f3bd5470db441c915f10dcee2c2750",
      "4f8cd4c814cc48ae826f6221ee73da34",
      "84ef5491e6d0442c93286cc409893802",
      "65a97bca9893405dad3b3f7679012585",
      "56fe75c12f8f44fdaaa1d8fabdde69c8",
      "bf9b7b1e04964d80b093c8e71e05035d",
      "2718c41125ee40dc98afa8c536d1991f",
      "05391752ed124c2db218c2d7e95a7c38",
      "ef9b7c8657b249d8b9f963b09476ce47",
      "5138a5444ce24577a7f6fd98d0e24397",
      "1b360d6b791741aa822bb46775b2c033",
      "a790b33a06c44b08ae28c8212c427d40",
      "a9864b9092aa41aca36e0495ce09f044",
      "c8dcd3b936a14677b785e65dacddd3e9",
      "e64ec0feb2be40e1b8a81bcc55371d30",
      "97de308e6ad4483798fda1a6bdd8549e",
      "b7a8f8ae468e453da24b4e31102362fe",
      "43dc192cbd4d4f2e8e5a4e48e194f606",
      "8abe90a944dd4c6a90903237a75e099e",
      "bd52c872117e4f1fa061617da931b241",
      "39ac4c529fef434bbaa1dd5da71a9c91",
      "ac6a34df872f4875bac683a1306f258c",
      "b8cacd987f8a45e6aa5d2aeb74ec3500",
      "eb31c38146274832871be7f42af91fc3",
      "6ec4ec80ae7e41debeb8de31d5260a47",
      "befcf2e7abcb4dc0b02da748f31fa6bd",
      "59f710d951b04ea8b3cebfa3a480aeca",
      "1f9869d8f3634bf18f6299d72d060045",
      "bcf8a447d21a4fdbba697582089acf01",
      "7edbe06fcf61450682f6d5eb8f4ecc34",
      "97d98c53372e4a44ad3304d583fffd7c",
      "69780cc76937443eb9995717cfbe2844",
      "bb7437cd7f1844b987d5645a7c70dd06",
      "2d7e03c2981044bfba4082ecbff743b6",
      "b719a5572d4c44dd8413d6b0c15e26d7",
      "89f27331c8e5432989ccf27865da1721",
      "8bb8b03f883f4d05b3c1816c03c6ff17",
      "8ef81a780e9b45f99f03b1ca3fb0215e",
      "bc921b43882e425fb303df971ccb5a13",
      "796951e182d54f4e8adbfd8960e415cc",
      "c99899916a244f87aecf140fe5df4fcf",
      "a0e466a220564c03b57738ae5d156f19",
      "d17725a9b5924247be387af597751ef3",
      "39e38eab75464ce89e8e37c9447dd502",
      "09df1faea80e46d890bdc253afb242b2",
      "c518b6a170fc4d34bbd9e735542d320f",
      "635f2b78889e43eb91cf565794718aa7",
      "8e304ddac9f3492cb9e96d52733144cf",
      "814fc642ffa449a09084aad7216bdc67",
      "3380635cac7c4e339bf894c3700990b6"
     ]
    },
    "id": "qXcN5GZg_ml2",
    "outputId": "7ea5e3c6-d9d4-4bab-938b-3e5e938f7102"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd9adda7d31640719195b4538b452662",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 16 files:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6840ced2b4b244c882e1207d74eb190a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/596 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30bb64f0950c41a4be940484e04abfc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de5c6ba4cc1a41c696f6c5c9e1a2a57e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.52k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c067e44d0c4474799f61dcdeea2e93b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/1.52k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59d3b98ddc8f43718d4290060f0bd763",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4cb8ed4b3004d2eaf29a6826761c5d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93e98717b6b14a80803ef3f70531ce5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c17b1ba827c405db80c7acfbc3650f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17f6945b7a8d4811bfa4ce0fffcd9457",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00003-of-00003.bin:   0%|          | 0.00/5.06G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4b84492006f4be0a90055fe40216808",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5eb949c92a34cc09599e161497f90ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce7a5d27a95944ec94b621fcd954883e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00001-of-00003.bin:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2718c41125ee40dc98afa8c536d1991f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00002-of-00003.bin:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43dc192cbd4d4f2e8e5a4e48e194f606",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcf8a447d21a4fdbba697582089acf01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "796951e182d54f4e8adbfd8960e415cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.10k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fnSaveModelandTokenizer(model_name, save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101,
     "referenced_widgets": [
      "ab7c10e0ac404948a39316dbecff9a8c",
      "743aa11eaaa54179a325d54222d4f941",
      "daed865856f54da7a448ec8adc4ff9b9",
      "8dfdc96cfcad41978148ca3b97429e80",
      "d0900ee8599f45f9b21f21d149a2e863",
      "4cfbc6782af54204a9295505027b2f5f",
      "302a4ee069da48be862c9c535b320546",
      "0fc0148edce44805afc2de9aecb04214",
      "a7a8d3dc50664a3abfdbd8de8cf5e0bd",
      "5fe978a2b5a94dfe9079835b870a8690",
      "c29a53cc204d47c88189c953a1738f57"
     ]
    },
    "id": "U0NYiRrsJOmu",
    "outputId": "ca53fc65-fd29-40d3-fe07-c51210b33ab9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n",
      "Tokenizer successfully loaded.\n",
      "Loading model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab7c10e0ac404948a39316dbecff9a8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, tokenizer = fnLoadModelandTokenizer(model_name, save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 563,
     "referenced_widgets": [
      "48627dc6012c4d57b085acf7ad2ee803",
      "e5bfeecb249942aaa85de0e703b08b0a",
      "13f1db0fae1f45ccb431bcdcd215fc17",
      "a0b9c2091b2b45248918b4159ea6a8f0",
      "afec67f979dc4fa2b0110a15eec6a004",
      "b6b0614e76134f8a913bdcc9f7133775",
      "2fcde7b14aa8406c86f85cef22f6fef8",
      "6b496587a92b4bc2a6cbb1cb8acb97b1",
      "31dd9f12d32a4e7a96e583341ec31462",
      "d3a9554c406d4068b8a2555e18524782",
      "7525268ffe014d3bb28d6014a81829a8",
      "e8739bf879474c85b45ab9e8aee00768",
      "2fcd298dbeb64431b261684bc234fef1",
      "f795f2662e5d4f68bd1407a63c00b0b3",
      "f6f699eeb33c413ab1d9c9a205aad6f7",
      "22d6a9cd290a4f07af418c818f45ed9e",
      "83efbba064f34eeeac323c0f6fe99449",
      "dfe8d1d8923f410d9461357cdfff9974",
      "70a87cf9bf764a0086acc7416d722813",
      "6469f9df1f8d49d98c6d585d0d7580a9",
      "d17195f0b6444b98bd87cc762f0d8ba5",
      "4a57c953d6e4422ba541c2589a89d4df",
      "bc7306787e8f4794bbbcd4180dac7a5b",
      "ed375cff08d849ce9ddb3d8d55342f3a",
      "cfca2cf626ad47c6893e5e93e5656458",
      "cf8c6cfc4eb246c9aefedf155fd896ea",
      "4ac614b92aad479eb670854357146e3f",
      "6f057d6a3f174e26bc231fbd7d2d9ade",
      "a11f532f701c4b0aa31655ef9cce6952",
      "750ed85def8e4085a742338bd0d4e08c",
      "dd5948314c03410fa60217c6ff0fe51f",
      "982f60cf64bf45269286f1db17099935",
      "fbcb0e9556f14d1da9122794910943a8",
      "9f268a1cf24d419aaadb77b3c37ce929",
      "b13f5d43be8544c18053eede0edff5ac",
      "5c1606a583374ec480f0f6d399ac825d",
      "d82168793fbe495499e2be91ed44c10a",
      "8bac44f76a484c17991749869b256832",
      "8e374c056e214767bdc141b97afcd6a0",
      "64b62850a1264df5865b13357d3519f0",
      "76d61ab9e9ca4490b0470831e1c85d4c",
      "e36d202274a848ddb5e020b7c4c8665c",
      "988f05b9fe1941cba836905bfc0e62cb",
      "6db250b8c609416d8ca0eab02bed61e5",
      "f1bae6ee8382413b8164adca1366230d",
      "ff9c7272bce34eeabd1d950d6b4cbe04",
      "b682774c32304701bff4ffc57381f0cd",
      "db97aa650f444d4d859148113694b2b7",
      "0579119ec2b14bb5969110ae469022f9",
      "1af5b895e1e042878011be28eb847e54",
      "a3097889aa0748448662892857479f3e",
      "a9624a853c324b0bbea9b3a69146d64a",
      "79175bcb40724b4eb713f110d562a77a",
      "5be675e29bc44a5a9d40b0af520b4773",
      "e962e9d55e9f4374b30c5d815d4be016",
      "8d6829372480438095fc281978904a9b",
      "0a08e8c5700046a3b506f596e09f235b",
      "b913ed69d7a345aa9e25106b80533d64",
      "481ab1ac4fc94f4099f0750de5bef11e",
      "85eba5c5ef984790b2062d289c829aaf",
      "922e137cf2024e3bb3560b53cebb92ca",
      "69673d2655e44ba89f97788b87dbf158",
      "a3bae1a6d4a94a72aec865e3c149f833",
      "1f6d3646ec15427cbea813b74dbbc7f6",
      "102d03302bcb47a1a06479df324e7bfd",
      "e05429bce749445986044bf27f5376ed",
      "a0721094d0374b6f9c985a743e782859",
      "6ad8aee417ec4963a3ded7025c0180e8",
      "e76c75c459df4fdd802b6a83ce45d731",
      "302f554ffdba41d58b94d04e001e5f79",
      "886c11fd917c4de8859665af3862e0c8",
      "f52c1246d39c4e0d89e136f0c821588e",
      "c354372434de4890ac642a9a1098a97c",
      "574618e57103493d95830f68095b7b34",
      "0742d4f8e5f84c9aacfafade9d3306d1",
      "7edf50e2814a48398bd5e3b7e950fc0a",
      "91f0389c201b451698cf771bc4861a21",
      "3634bd5d1f6440b1a9c1784e738a5599",
      "8cc99ca9087d4ccd88aab04429584267",
      "99f6067fa48a4f8baed79a77e4ab24e3",
      "50defaf6c6144e9fba5c974c15191894",
      "3267627e5d8e4b639fd2d1496aef3f82",
      "744f3e9273474629928fda387c33d2da",
      "7dbbf281b47e49c8b482ef5c6b411552",
      "aefa17b793734fe4948afe2a182e0ae9",
      "03bf6dfa9f5d4f8a8193c6369c61a4c9",
      "66e6f9c2169c4b29ba11ebd651f2edf5",
      "97de491128184d04b425656bdea33716",
      "b7aa5dacf8b1420b8abec279d5999cbb",
      "31b37499b51b4deb911930e598947960",
      "63f26c7c90af436681f77bd32ce374d6",
      "05699c326bbc44a2a6b3fe01c0eb2841",
      "4ed6f551cc4d4051868b14d4945e3ae8",
      "07e06a2b54cc4f308c095af46b8388b5",
      "be6a2c74c3084c3998b7fc0c5885d9a8",
      "09ea0673dc364a76902ef3d60eadd45b",
      "045f2dd4c47c4767a1707fb314c6a5c3",
      "f355a0fd5d9f4518870a6b8d5ff46de9",
      "2bbec77894ae4c1182f2e7768a91f7a2",
      "f7c0c13685de49a9a93dbbc4dfcc3497",
      "dc774652201b458d992cbd1eb2129506",
      "4bfa36bdc7e34b5998b2d06df8c3da77",
      "44e3c88de4724d31bb90cfa66961a90a",
      "fc103f279780425eb320b57e6d9860bc",
      "ba3e2ca456364fa3b4002b902705b66b",
      "eadef21a0eec4fe89010936d7331539b",
      "5108fad9513541bd88137c9778b53d6d",
      "d56cef5ec4e84d8c9f6120ec64271cb2",
      "2345e9dc33034807af5d2cf0a87a44c8",
      "63691372404b4b8db8068498f189fa37",
      "9220070d544b48fb921b53665c41aab8",
      "8e9689736ae5454c950d2489840653ed",
      "9f648b7b7216474a84f005f2730d1061",
      "ac24f7719f2048469c4469c773a8fb01",
      "ce55dabe18e64ed29ba1de495a6044f8",
      "075fd928f65847aab08e15bb90626b3b",
      "b83420410281452f80bf8f14af879029",
      "fec9d5b82cc044dcbbfdf1d71c0c60f4",
      "bb5001b9a6044fa88239182026e37554",
      "23b8c60ddbf642b5bff8bbd37bb4abfd",
      "dd593f1788fe4862bca3ecca81884484"
     ]
    },
    "id": "qSWE1NeyJOkE",
    "outputId": "52315d88-fd8a-4598-8354-c71e33c28900"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF Read Successfully\n",
      "The document has 83 pages.\n",
      "The document has 78 pages after removing irrelevant content.\n",
      "Text Cleaning completed\n",
      "The document has 612 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-707d668d35c8>:98: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48627dc6012c4d57b085acf7ad2ee803",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8739bf879474c85b45ab9e8aee00768",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc7306787e8f4794bbbcd4180dac7a5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f268a1cf24d419aaadb77b3c37ce929",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1bae6ee8382413b8164adca1366230d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d6829372480438095fc281978904a9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0721094d0374b6f9c985a743e782859",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3634bd5d1f6440b1a9c1784e738a5599",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7aa5dacf8b1420b8abec279d5999cbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7c0c13685de49a9a93dbbc4dfcc3497",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9220070d544b48fb921b53665c41aab8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VectorDB created and saved successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-707d668d35c8>:100: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectorstore.persist()\n"
     ]
    }
   ],
   "source": [
    "vector_store = fnCreateVectorDB(file_path, persist_directory, pages_to_exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "uSt-OKL-JC_c"
   },
   "outputs": [],
   "source": [
    "# compression_retriever = create_compression_retriever(model, tokenizer, vector_store)\n",
    "\n",
    "vectordb_retriever = create_vector_retriever(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5g67F1t-JLpk",
    "outputId": "b74731c8-141a-4817-ba68-b900b42fa63e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] Use the following pieces of context to answer the question at the end in ONE SENTENCE ONLY.\n",
      "If the context is not relevant to the question, then Helpful Answer : I don't know the answer to that\". Do NOT give any explanation.\n",
      "Don't try to make up an answer. Avoid Repetition.\n",
      "\n",
      "Context: {context}\n",
      "\n",
      "Question: {question}\n",
      "Helpful Answer: [/INST]'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compression retriever is returning a lot of junk\n",
    "# qa_chain = create_qa_chain(model, tokenizer, compression_retriever)\n",
    "qa_chain = create_qa_chain(model, tokenizer, vectordb_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "0cS6xcDz2OCe"
   },
   "outputs": [],
   "source": [
    "questions = [ \"What are the core values outlined in the Morgan Stanley UK Employee Handbook?\",\n",
    "    \"What is the notice period required for Professional Employees to terminate their employment?\",\n",
    "    \"What is the notice period for Vice Presidents, Executive Directors, and Managing Directors?\",\n",
    "    \"How many days of annual leave are all Professional Employees entitled to?\",\n",
    "    \"How many days of annual leave are Vice Presidents entitled to?\",\n",
    "    \"How many days of annual leave are Executive Directors entitled to?\",\n",
    "    \"How many days of annual leave are Managing Directors entitled to?\",\n",
    "    \"What additional service-related annual leave entitlement is provided based on years of service?\",\n",
    "    \"What is the procedure for notifying your manager about annual leave?\",\n",
    "    \"What is the maximum number of annual leave days that can be carried over?\",\n",
    "    \"What is the dress code policy for employees at Morgan Stanley?\",\n",
    "    \"What constitutes appropriate business casual attire as per the handbook?\",\n",
    "    \"What is Morgan Stanleyâ€™s policy on smoking within its premises?\",\n",
    "    \"What guidelines are provided regarding substance abuse at Morgan Stanley?\",\n",
    "    \"What are the rules for using internet and electronic communications at Morgan Stanley?\",\n",
    "    \"What policies govern the use of firm systems?\",\n",
    "    \"What does the confidentiality clause require from employees during and after employment?\",\n",
    "    \"What responsibilities do employees have regarding intellectual property created while employed?\",\n",
    "    \"What does the handbook state about data protection and privacy?\",\n",
    "    \"How should employees keep their personal details updated according to the handbook?\",\n",
    "    \"What are the guidelines on outside interests and external employment?\",\n",
    "    \"What are the rules regarding employees' use of social media?\",\n",
    "    \"What is the procedure for reporting sickness absence?\",\n",
    "    \"How is sick pay determined as described in the handbook?\",\n",
    "    \"What does the handbook specify about mandatory vacation requirements?\",\n",
    "    \"How many consecutive weeks of annual leave are employees generally allowed to take?\",\n",
    "    \"What is the process for handling disciplinary procedures at Morgan Stanley?\",\n",
    "    \"What steps are outlined in the grievance procedure?\",\n",
    "    \"How are deductions from remuneration handled in the handbook?\",\n",
    "    \"What are the terms and conditions regarding fixed compensation and base salary?\",\n",
    "    \"How is the base salary calculated for employees who start mid-month?\",\n",
    "    \"What is required of employees in terms of obtaining and providing references?\",\n",
    "    \"What does the handbook state about overtime payments?\",\n",
    "    \"What rights do employees have under the confidentiality and intellectual property clauses?\",\n",
    "    \"What provisions are made for employee benefits?\",\n",
    "    \"What is the policy regarding family leave (maternity, paternity, adoption, etc.)?\",\n",
    "    \"How are collective agreements addressed in the handbook?\",\n",
    "    \"What guidelines are provided regarding equal opportunities?\",\n",
    "    \"What does the handbook state about dignity at work and harassment?\",\n",
    "    \"What is the process for suspending an employee under the handbook?\",\n",
    "    \"Under what conditions can an employee be dismissed without notice?\",\n",
    "    \"What is the procedure for returning Morgan Stanley property upon termination?\",\n",
    "    \"What guidelines are given for reimbursement of business-related expenses?\",\n",
    "    \"What does the handbook say about health and safety at work?\",\n",
    "    \"What are the provisions for flexible working arrangements?\",\n",
    "    \"What is the process for reporting health issues as per the handbook?\",\n",
    "    \"How does the handbook define and manage outside interests?\",\n",
    "    \"What standards of conduct are expected as per the Code of Conduct?\",\n",
    "    \"What responsibilities do Senior Managers have under the regulatory rules?\",\n",
    "    \"What is the procedure for the annual fitness and propriety assessment?\",\n",
    "\n",
    "    # 15 generic questions outside the handbook context\n",
    "    \"What is the capital city of France?\",\n",
    "    \"Who is the current President of the United States?\",\n",
    "    \"What is the tallest mountain in the world?\",\n",
    "    \"What is the fastest land animal?\",\n",
    "    \"How many continents are there on Earth?\",\n",
    "    \"What is the boiling point of water in Celsius?\",\n",
    "    \"Who wrote 'Romeo and Juliet'?\",\n",
    "    \"What is the largest ocean on Earth?\",\n",
    "    \"How many planets are in our solar system?\",\n",
    "    \"What is the chemical symbol for gold?\",\n",
    "    \"What is the speed of light in a vacuum?\",\n",
    "    \"Who invented the telephone?\",\n",
    "    \"What is the population of Tokyo?\",\n",
    "    \"What is the national language of Brazil?\",\n",
    "    \"What is the primary ingredient in traditional Japanese miso soup?\"\n",
    "]\n",
    "\n",
    "dfQandA = pd.DataFrame(questions, columns=[\"question\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ofvGLmLy2N_C",
    "outputId": "afb3968e-ac6f-4579-fcb8-18d8652b2c8f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing questions:   0%|          | 0/65 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Processing questions:   2%|â–         | 1/65 [00:03<03:32,  3.32s/it]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Processing questions:   3%|â–Ž         | 2/65 [00:09<05:24,  5.15s/it]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Processing questions:   5%|â–         | 3/65 [00:16<05:57,  5.77s/it]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Processing questions:   6%|â–Œ         | 4/65 [00:18<04:20,  4.27s/it]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Processing questions:   8%|â–Š         | 5/65 [00:22<04:06,  4.11s/it]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Processing questions:   9%|â–‰         | 6/65 [00:23<03:16,  3.33s/it]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Processing questions:  11%|â–ˆ         | 7/65 [00:25<02:38,  2.73s/it]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Processing questions:  12%|â–ˆâ–        | 8/65 [00:29<03:01,  3.18s/it]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Processing questions:  14%|â–ˆâ–        | 9/65 [00:31<02:43,  2.91s/it]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Processing questions:  15%|â–ˆâ–Œ        | 10/65 [00:34<02:43,  2.98s/it]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Processing questions:  17%|â–ˆâ–‹        | 11/65 [00:37<02:27,  2.73s/it]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Processing questions:  18%|â–ˆâ–Š        | 12/65 [00:42<03:09,  3.58s/it]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Processing questions:  20%|â–ˆâ–ˆ        | 13/65 [00:45<03:00,  3.46s/it]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Processing questions:  22%|â–ˆâ–ˆâ–       | 14/65 [00:49<03:02,  3.57s/it]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Processing questions:  23%|â–ˆâ–ˆâ–Ž       | 15/65 [00:54<03:14,  3.89s/it]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Processing questions:  25%|â–ˆâ–ˆâ–       | 16/65 [00:56<02:41,  3.30s/it]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Processing questions:  26%|â–ˆâ–ˆâ–Œ       | 17/65 [00:59<02:44,  3.43s/it]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Processing questions:  28%|â–ˆâ–ˆâ–Š       | 18/65 [01:03<02:39,  3.40s/it]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Processing questions:  29%|â–ˆâ–ˆâ–‰       | 19/65 [01:06<02:35,  3.39s/it]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Processing questions:  31%|â–ˆâ–ˆâ–ˆ       | 20/65 [01:14<03:36,  4.80s/it]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Processing questions:  32%|â–ˆâ–ˆâ–ˆâ–      | 21/65 [01:17<03:09,  4.30s/it]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Processing questions:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/65 [01:21<03:01,  4.22s/it]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Processing questions:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/65 [01:29<03:35,  5.13s/it]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Processing questions:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 24/65 [01:37<04:06,  6.00s/it]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Processing questions:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 25/65 [01:41<03:40,  5.52s/it]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Processing questions:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/65 [01:43<02:51,  4.40s/it]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Processing questions:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/65 [01:45<02:25,  3.82s/it]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Processing questions:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 28/65 [01:49<02:21,  3.82s/it]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Processing questions:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 29/65 [01:53<02:17,  3.81s/it]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Processing questions:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 30/65 [01:56<02:00,  3.45s/it]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Processing questions:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 31/65 [02:00<02:08,  3.79s/it]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Processing questions:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 32/65 [02:04<02:02,  3.71s/it]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Processing questions:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 33/65 [02:07<01:51,  3.50s/it]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Processing questions:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 34/65 [02:11<01:56,  3.75s/it]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Processing questions:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 35/65 [02:13<01:36,  3.22s/it]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Processing questions:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 36/65 [02:16<01:30,  3.10s/it]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Processing questions:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 37/65 [02:17<01:13,  2.61s/it]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Processing questions:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 38/65 [02:21<01:15,  2.79s/it]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Processing questions:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 39/65 [02:25<01:23,  3.21s/it]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Processing questions:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 40/65 [02:29<01:25,  3.43s/it]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Processing questions:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 41/65 [02:32<01:23,  3.46s/it]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Processing questions:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 42/65 [02:35<01:13,  3.20s/it]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Processing questions:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 43/65 [02:39<01:17,  3.51s/it]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Processing questions:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 44/65 [02:41<01:06,  3.16s/it]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Processing questions:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 45/65 [02:44<00:58,  2.93s/it]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Processing questions:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 46/65 [02:46<00:53,  2.81s/it]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Processing questions:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 47/65 [02:50<00:56,  3.12s/it]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Processing questions:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 48/65 [02:53<00:51,  3.05s/it]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Processing questions:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 49/65 [02:55<00:44,  2.78s/it]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Processing questions:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 50/65 [03:02<01:00,  4.01s/it]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Processing questions:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 51/65 [03:05<00:53,  3.81s/it]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Processing questions:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 52/65 [03:08<00:45,  3.47s/it]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Processing questions:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 53/65 [03:10<00:37,  3.12s/it]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Processing questions:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 54/65 [03:12<00:29,  2.64s/it]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Processing questions:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 55/65 [03:14<00:23,  2.37s/it]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Processing questions:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 56/65 [03:15<00:19,  2.16s/it]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Processing questions:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 57/65 [03:18<00:18,  2.29s/it]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Processing questions:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 58/65 [03:19<00:14,  2.00s/it]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Processing questions:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 59/65 [03:23<00:14,  2.46s/it]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Processing questions:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 60/65 [03:25<00:11,  2.26s/it]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Processing questions:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 61/65 [03:26<00:08,  2.08s/it]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Processing questions:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 62/65 [03:29<00:07,  2.41s/it]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Processing questions:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 63/65 [03:31<00:04,  2.07s/it]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Processing questions:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 64/65 [03:34<00:02,  2.32s/it]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Processing questions: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 65/65 [03:35<00:00,  3.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 24s, sys: 5.97 s, total: 3min 30s\n",
      "Wall time: 3min 35s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dfQandA = answer_questions_from_dataframe(qa_chain, dfQandA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "qD6YO3l58HZE"
   },
   "outputs": [],
   "source": [
    "dfQandA.to_excel(\"RAG_Questions_and_Answers.xlsx\", index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
