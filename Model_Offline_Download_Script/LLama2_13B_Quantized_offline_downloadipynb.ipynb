{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O6ASiK-dzEPQ"
   },
   "source": [
    "<h5>Installing Necessary libraries </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Td2nns4FzZ5L"
   },
   "outputs": [],
   "source": [
    "!pip install transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qq0ygfff3XWh",
    "outputId": "48dccd99-491f-4334-b6fb-fbce1b396202"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m73.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m95.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install bitsandbytes -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "65xdE_HSA6gw"
   },
   "source": [
    "<h5> Importing libraries </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "-OZRi_GKzfH7"
   },
   "outputs": [],
   "source": [
    "# Importing libraries to load the Mode, tokenizer, and specify the QuantizationConfiguration\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "4wdbXfXZz9MG"
   },
   "outputs": [],
   "source": [
    "# We will be using llama-2-13b from Huggingface\n",
    "model_name = \"meta-llama/Llama-2-13b-chat-hf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uJNnmYt7Axyb"
   },
   "source": [
    "<h5> Quantization Config </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "eP7LQMX90C2u"
   },
   "outputs": [],
   "source": [
    "# Specifying the quantization config\n",
    "# load_in_4bit -> Loads the model weights in 4 bit precision instead of 16 or 32 bits\n",
    "# bnb_4bit_use_double_quant -> Use double quantization - 1st Quantization reduces the weights to 4 bit using scaling factors. 2nd Quantization\n",
    "#                              quantizes the auxiliary data such as scaling factors and codebooks that are used for\n",
    "#                              mapping the original values (full model size) to quantized values.\n",
    "# bnb_4bit_quant_type=\"nf4\" -> Normal Float 4 which is a method for 4 bit quantization\n",
    "# bnb_4bit_compute_dtype=torch.float16 -> Datatype used during computation (format gets acceleration on GPU)\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # Load the model in 4-bit precision\n",
    "    bnb_4bit_use_double_quant=True,  # Use double quantization for better precision\n",
    "    bnb_4bit_quant_type=\"nf4\",  # Use 4-bit NormalFloat quantization\n",
    "    bnb_4bit_compute_dtype=torch.float16  # Use FP16 for computation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XKuIrixz7QqV"
   },
   "source": [
    "The model requires authentication via HuggingFace. So using the hugging face token and logging in using the token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "7DQPXkOH7YsB"
   },
   "outputs": [],
   "source": [
    "hf_token = '<enter your hf token here>'\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(token=hf_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mHWBQRSs9Zai"
   },
   "source": [
    "<h3>Different Model Classes and when to use what? </h3> <br>\n",
    "<ol>\n",
    "  <li> AutoModel - Use when: You need raw hidden states or plan to build and attach your own task-specific layers </li>\n",
    "  <li> AutoModelForCausalLM - Use when: You’re performing generative tasks, such as chatbots that generate text (e.g., GPT-style models). </li>\n",
    "  <li> AutoModelForSeq2SeqLM - Use when: You’re working on tasks like summarization, translation, or any generation tasks that require converting one sequence into another (e.g., T5, BART). </li>\n",
    "  <li> AutoModelForMaskedLM - Use when: You’re performing fill-in-the-blank tasks or working with models like BERT that are pretrained using a masked language modeling objective. </li>\n",
    "  <li> AutoModelForSequenceClassification - Use when: You need to classify entire sequences (e.g., sentiment analysis, spam detection, topic categorization). </li>\n",
    "  <li> AutoModelForQuestionAnswering - Use when: You’re building a system to extract answers from a passage (e.g., SQuAD-style question answering). </li>\n",
    "  <li> AutoModelForTokenClassification - Use when: You’re handling tasks such as Named Entity Recognition (NER) or Part-of-Speech (POS) tagging. </li>\n",
    "  <li> AutoModelForMultipleChoice - Use when: You’re dealing with multiple-choice questions, common in certain reading comprehension tasks. </li>\n",
    "  </ol>\n",
    "  <br>\n",
    "  <h5> As the end goal is to create a RAG powered Chatbot, we need a model to generate conversational responses. Hence, AutoModelForCausalLM is the best choice </h5>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O-koDlzg-5Ld",
    "outputId": "29152527-1d49-4b6b-c2c4-a49c24306c9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.3)\n",
      "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.5.1+cu124)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (1.26.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2xcCwtMqBqxk"
   },
   "source": [
    "<h5> Downloading the model and Tokenizer </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 595,
     "referenced_widgets": [
      "027cf63e67b64967a124ff3829855b2f",
      "802cf585d9af4ba89f27702c24c0e1ef",
      "e55924045d824da98c4a0339c8967289",
      "fd0e272ecb394b9b93917df6143f532f",
      "b23d4f41bdc04891bbdd0aa42acfdb38",
      "85a5fa75ed564b55a0587dc7f9aa43c3",
      "9d2beede12b64ad98adc096de92555b3",
      "20705206d45847ebb1748b065418474a",
      "44fe496273674127ae18a0ea7203f03d",
      "2216ed25556f4cd88e0ca5b2f6f0668e",
      "b877827ac5454f6f8f53edbb27028c53",
      "ca2902c39fdb4b4ca9d0fde5b27d40c5",
      "de8c5382d1cf41369064e787c5840f2a",
      "0def6eb0201d4a89ad8192e5c44b6158",
      "90242b3bc1a344bc88ae68c095e2eefc",
      "6cbca45df8ba45fa846d6109b7f69254",
      "137dc27139bd4fd6830170f28658161e",
      "84a7b4372b9d40248e29259d8cbfdca8",
      "ca0a018097d14c48bf452585b986115a",
      "07af7f6c2a3144858f778f5b01547dc8",
      "2b9f3a748c9f4c3599e79496f0f84541",
      "c3774e2c084f49f4a312c6c1937f0062",
      "70be990c5a8f40ab8d5472e9bb98d786",
      "bcac04f46133497eb63075f120d939b6",
      "c2f48ce7137742b39bef125555003343",
      "e1f65893e70b4aeb9ea3fe6fce171fd3",
      "35ce1de5356a43a7aac3885e116bc407",
      "c7b9de6164894faa9e83c6e9616663b1",
      "87f13398b51a497fa5b4a2b2a19acbbe",
      "1b9d2ce624dd4c3b80f14d7fd47353a6",
      "ba301b20e0ae4c2199856b28a4060cb3",
      "0c6e0aee4748472dbab939eb859e0760",
      "e9a8d3895adc4e4081260cca268424b3",
      "a4927a30cf5747058d679df5785355e8",
      "354dc0e03c4b4c7eaa0cad34e6b30af4",
      "db869cbb683941419635f092fa2218df",
      "f25ff6fa03f34bae8e9083c05f9f7ab5",
      "025ad5a717fb4a53b1cff9a780a835ce",
      "8755f1734e734584b9ac13fe4b819a88",
      "95847260a4d940769e3e3bd2dd366c54",
      "b70fdf6fc6b14cd8937cc13be97c66d7",
      "8372cf8c3efa4d388d615b8027b53958",
      "d91ec7a0435e4adfacdf76f4c74f15cd",
      "dd2fdb074db04a9a8c68ae27b90e11c5",
      "ae151d3242cf4f4083929f9b839a7b30",
      "fc83243b3b444824846e0cf2289615b3",
      "a3ec9d616ad642e1af96e2e8d6751a08",
      "92eaa58819824235840118eec6b462e5",
      "f5073e43bb8348918d280d1fa2b26816",
      "b31863791c12412bbfc09992a72f7bd6",
      "f3604163fc2f42a68998321f1fa7c83a",
      "a64316f49d434f3db5087ecf6a4c0649",
      "2b7e3087bee34393bc69a8c158ce5747",
      "f4a27957ddad49e4ba32259bc37aec47",
      "10b276c0656145fb882b6e267e3ce2d0",
      "63f144f2e1394988bbceff43511a7d7e",
      "0b247a70231745c79948e1c389c27f2c",
      "07e0a5cc0ff449bcb644c1e01c5577ee",
      "4cf45443eb0e46e5b2d15a56ec906b82",
      "f57577d20b9c45e9b2abef278ea47d95",
      "31ab024bf6c642c0ada9aaea913f522f",
      "48d648722dbc43a1aadea0b4e3e2e067",
      "06880b3557024b70ab4da7de1faef732",
      "3901edbf87cc42c786197b42525d94c5",
      "6996edd5bc3547238b0e63a3d77dcd0a",
      "46550a3e37e742af8464385b53163f50",
      "6e7d3967f1614793952e9117a1204af5",
      "ff1b0dcf15d34b7bac94279033d53c8b",
      "419256908a334c1c86ce8f43a1da911f",
      "afb9fbbd94864083ad6d669282702ad9",
      "62a3976e10b54aae8060eb5feef131fd",
      "cce8ab3be3e1413f9d68f7d2918c8ed1",
      "e0d2130405b1439baaf20d4e273cdc13",
      "b5116cd5c07046aa96359ff995445349",
      "06e490b7c7b04b4a9f1d1ce384efdbad",
      "df89973887294210a50176e0bcaa3322",
      "cd2fe86576bf4fb1951348e4f8d34da5",
      "a83b8dd493e24350a4a6e2592d226c60",
      "22bd3b6b2cc04ff4b72c7bef71251b29",
      "b75de4b3a2594bf3a208a6b2b49fcede",
      "0247abc553894455ae16b30ecb5c6cdf",
      "afa2f6b8c4224aaa963f8038dea61021",
      "13d5596cfcfd48e8accf56d7b5f0a54d",
      "7fa21f6ed7c64b08b171ef75609c9c59",
      "6a4d3f88a88445f28edf9d0953dea1c5",
      "a1fdfa1bf71e404e9e64986d7aa249bb",
      "85e3cc247a0942e686070dc3a8b190d5",
      "d4e303ba16a14f32aeae63e79c161d5e",
      "1e043c26d74141fab05e67ec400b7c8f",
      "627f99a1cacf49b7b112cb9d6a8a59a8",
      "cd3ec72308af4d499d16cdc775454e23",
      "4a683ba071c04b37a5182090f9c37366",
      "bb097b0494a9432bbcfcdbedf54ec6b2",
      "31a155ae222c45418b57ae3641a15900",
      "c28e302cc2b54bf38a0251d7561245c3",
      "4b32bd7d998e42dc9bcc1b6c8448a5dc",
      "cb8567eba14f4241ae4b58d04d039634",
      "a8696a5b2bca4ce3b350953adc7ace65",
      "001bbe5bf24b490e857a77bdf4bacd5e",
      "7add033c785847c19a0730912034082b",
      "11ecfb43bc06408186e607d3540aba4c",
      "e25c9d07751a44c8910397538de9db3d",
      "11f197729f0f4fbeb2efe0c80d9861b5",
      "3c8d7450e03a42a38ac271da2d10c969",
      "c6b062edf3824f7493eb362191f71c60",
      "d55ac7f749c84b59a3048fb61596ffef",
      "7c8c7a650bf446a78f032b05537554fb",
      "d0da871819ce4fa582f5f6adfad21ac2",
      "8f3d144b56cd45418149d6adea3a9c90",
      "d6829a5bccbb4993bac6c5ae6cbeab07",
      "a2e106890fbf438bb62c7ec97b155039",
      "ccd8005e8601443699697caf7632f034",
      "1cdf7fec36a640d59d581cd4bb39ebff",
      "ced4f8087431442da9f142b2e58a612d",
      "c29abea8cf434d25973e4cf04c9072d7",
      "0e7773cb513741618c71a14a7be8bbcc",
      "c42f11c2cea64263ab9c955ecffc2033",
      "66626d7f4a9e4368a55b9a9a26791c7a",
      "e51f4d3d21294a6eb33252029a4a4848",
      "9f1fb9d47ffe4f8591f2c6171d48c122",
      "2aa583b7f9ee48d9a223c6ec805803b7",
      "92a81090f9c0474e99b45bc4824d7bba",
      "8d37d3ed782b41dca9c193e6beb32bd4",
      "843baf51f8e340f58284c75f5bb4bce4",
      "fb60f4ad333749efae0d278883fa3046",
      "fc56c173b7234308bceb1aa2eed124c0",
      "284138f6abaf4a2386d013764e30990c",
      "7d21a4205df04ed1978a86811d582067",
      "811545a9214b41c5ad18710a7246a214",
      "f01c83960f134e699e8c3c94484d0e8e",
      "8fe62e6d44b44f5eabaedb7e4131e7c7",
      "c2f345ad28474ec09367fe54c08c1ea1"
     ]
    },
    "id": "8sIewL_U3VzJ",
    "outputId": "758855e1-4fc6-474d-f7b4-faec262fe805"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downmloading tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "027cf63e67b64967a124ff3829855b2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.62k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca2902c39fdb4b4ca9d0fde5b27d40c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70be990c5a8f40ab8d5472e9bb98d786",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4927a30cf5747058d679df5785355e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer successfully downloaded.\n",
      "Downloading model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae151d3242cf4f4083929f9b839a7b30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/587 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63f144f2e1394988bbceff43511a7d7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/33.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e7d3967f1614793952e9117a1204af5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a83b8dd493e24350a4a6e2592d226c60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/9.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e043c26d74141fab05e67ec400b7c8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/9.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7add033c785847c19a0730912034082b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/6.18G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2e106890fbf438bb62c7ec97b155039",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92a81090f9c0474e99b45bc4824d7bba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model successfully downloaded.\n"
     ]
    }
   ],
   "source": [
    "# Downloading the model and the tokenizer\n",
    "print(\"Downmloading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(\"Tokenizer successfully downloaded.\")\n",
    "\n",
    "print(\"Downloading model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=quantization_config, device_map=\"auto\")\n",
    "print(\"Model successfully downloaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C6hCMWlsBxT6"
   },
   "source": [
    "<h5> Dumping this model to disk </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SKgdwRqP7IIF",
    "outputId": "23ae7cb5-18dd-4770-de5b-4fba94d2beda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model and tokenizer to disk...\n",
      "Model and tokenizer saved to './llama2-13b-chat-4bit'\n"
     ]
    }
   ],
   "source": [
    "print(\"Saving model and tokenizer to disk...\")\n",
    "model.save_pretrained(\"./llama2-13b-chat-4bit\")\n",
    "tokenizer.save_pretrained(\"./llama2-13b-chat-4bit\")\n",
    "print(\"Model and tokenizer saved to './llama2-13b-chat-4bit'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V_qniWsfCrZj",
    "outputId": "4d45b68c-80c0-4291-bc88-aa3a3c8d928c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: content/llama2-13b-chat-4bit/ (stored 0%)\n",
      "  adding: content/llama2-13b-chat-4bit/model-00001-of-00002.safetensors (deflated 4%)\n",
      "  adding: content/llama2-13b-chat-4bit/tokenizer.json (deflated 85%)\n",
      "  adding: content/llama2-13b-chat-4bit/special_tokens_map.json (deflated 74%)\n",
      "  adding: content/llama2-13b-chat-4bit/config.json (deflated 55%)\n",
      "  adding: content/llama2-13b-chat-4bit/tokenizer_config.json (deflated 66%)\n",
      "  adding: content/llama2-13b-chat-4bit/model-00002-of-00002.safetensors (deflated 6%)\n",
      "  adding: content/llama2-13b-chat-4bit/model.safetensors.index.json (deflated 96%)\n",
      "  adding: content/llama2-13b-chat-4bit/generation_config.json (deflated 32%)\n",
      "  adding: content/llama2-13b-chat-4bit/tokenizer.model (deflated 55%)\n"
     ]
    }
   ],
   "source": [
    "# Zipping the model\n",
    "!zip -r my_model.zip /content/llama2-13b-chat-4bit/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "EmoLachIEzms",
    "outputId": "b4b5be7d-65f5-4059-9ce0-8e87b1cec834"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_a80fd32c-b870-4989-ac4e-ff74f120c253\", \"my_model.zip\", 6868048781)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from google.colab import files\n",
    "files.download(\"my_model.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FvF7gg_7GpgW"
   },
   "source": [
    "<h5> Inferencing on a basic question to see if the model fits in Colab </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "74QEgWWlGmwg",
    "outputId": "b28540d5-a23e-46ad-80e7-7b67a3238816"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=50) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: [INST] What is the capital of Djibouti? [/INST]  There is no country called \"Djibouti\". Djibouti is a city and a province in Eritrea, and it does not have a capital. Eritrea's capital is Asmara.\n"
     ]
    }
   ],
   "source": [
    "def ask_question(question):\n",
    "    input_text = f\"[INST] {question} [/INST]\"\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(input_ids, max_new_tokens = 50, max_length=512)\n",
    "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "question = \"What is the capital of Djibouti?\"\n",
    "answer = ask_question(question)\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5zDTTTVgHKVK"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
